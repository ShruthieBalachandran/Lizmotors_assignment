{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee55b12-16cc-40a8-af6c-8856ab5f3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio...\n",
      "Applying VAD...\n",
      "Transcribing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabha\\anaconda3\\Lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text:  How do you define success and do you feel successful in your life right now?\n",
      "Querying LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f5af7c89bb4a66aed7ce55f320951a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:  How do you define success and do you feel successful in your life right now?\n",
      " hopefully, your answer is yes. If not, keep reading.\n",
      "The fact is that most of us are not living the life we want.\n",
      "Converting text to speech...\n",
      "File saved successfully at: C:\\Users\\sabha\\output.mp3\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import webrtcvad\n",
    "import numpy as np\n",
    "import whisper\n",
    "import librosa\n",
    "import torch\n",
    "import asyncio\n",
    "import edge_tts\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "### STEP 1: Voice-to-Text Conversion (with VAD)\n",
    "def apply_vad(audio, sr, vad_level=2):\n",
    "    \"\"\"\n",
    "    Applied Voice Activity Detection (VAD) to isolate speech from the input audio.\n",
    "\n",
    "    Parameters:\n",
    "    - audio: numpy array of audio samples\n",
    "    - sr: sample rate of the audio\n",
    "    - vad_level: aggressiveness of the VAD (0-3)\n",
    "\n",
    "    Returns:\n",
    "    - vad_audio: bytes object containing audio frames classified as speech\n",
    "    \"\"\"\n",
    "    # Validated VAD level\n",
    "    if not (0 <= vad_level <= 3):\n",
    "        raise ValueError(\"vad_level must be between 0 and 3.\")\n",
    "\n",
    "    vad = webrtcvad.Vad(vad_level)\n",
    "    audio_bytes = (audio * 32767).astype(np.int16).tobytes()  # Converted to 16-bit PCM\n",
    "    active_frames = []\n",
    "    frame_duration = 10  # ms\n",
    "    frame_size = int(sr * frame_duration / 1000)  # Frame size in samples\n",
    "    \n",
    "    for i in range(0, len(audio_bytes), frame_size * 2):\n",
    "        frame = audio_bytes[i:i + frame_size * 2]\n",
    "        if len(frame) == frame_size * 2 and vad.is_speech(frame, sr):\n",
    "            active_frames.append(frame)\n",
    "    \n",
    "    if not active_frames:\n",
    "        raise RuntimeError(\"No speech detected in the audio.\")\n",
    "    \n",
    "    return b''.join(active_frames)\n",
    "\n",
    "\n",
    "def transcribe_with_whisper(vad_audio, sr):\n",
    "    \"\"\"\n",
    "    Transcribed speech from VAD-processed audio using Whisper.\n",
    "\n",
    "    Parameters:\n",
    "    - vad_audio: bytes object containing VAD-processed audio\n",
    "    - sr: sample rate of the audio\n",
    "\n",
    "    Returns:\n",
    "    - transcription: Transcribed text from the audio\n",
    "    \"\"\"\n",
    "    model = whisper.load_model(\"base\")\n",
    "\n",
    "    # Converted bytes back to numpy array for Whisper model processing\n",
    "    audio_tensor = torch.from_numpy(np.frombuffer(vad_audio, dtype=np.int16).astype(np.float32) / 32768.0)\n",
    "    result = model.transcribe(audio_tensor.numpy(), fp16=False)\n",
    "    \n",
    "    if not result['text'].strip():\n",
    "        raise RuntimeError(\"Transcription failed. No text detected.\")\n",
    "    \n",
    "    return result['text']\n",
    "\n",
    "\n",
    "### STEP 2: Text Input into LLM\n",
    "def query_llm(text_input):\n",
    "    \"\"\"\n",
    "    Queried the LLM with transcribed text and generated a response.\n",
    "\n",
    "    Parameters:\n",
    "    - text_input: Transcribed text to be input to the LLM\n",
    "\n",
    "    Returns:\n",
    "    - restricted_response: The response from the LLM restricted to 2 sentences\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Configured the model loading with memory optimization\n",
    "    bnb_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "    model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "\n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Restricted the output to 2 sentences\n",
    "    sentences = response.split('. ')\n",
    "    restricted_response = '. '.join(sentences[:2]) + '.'\n",
    "    \n",
    "    return restricted_response\n",
    "\n",
    "\n",
    "### STEP 3: Text-to-Speech Conversion with Tunable Parameters\n",
    "async def text_to_speech(text, output_file, pitch=1.0, speed=1.0, voice=\"en-US-JennyNeural\"):\n",
    "    \"\"\"\n",
    "    Converted the input text to speech with adjustable parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - text: The text to be converted to speech\n",
    "    - output_file: Path to the output file where speech will be saved\n",
    "    - pitch: The pitch adjustment (default is 1.0 for normal pitch)\n",
    "    - speed: The speed adjustment (default is 1.0 for normal speed)\n",
    "    - voice: The voice profile to use for TTS\n",
    "    \n",
    "    Output:\n",
    "    - Saved the audio file at the specified path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Converted the speed to a valid string format for the rate, e.g., \"+10%\" or \"-10%\"\n",
    "        rate_percentage = f\"{'+' if speed > 1.0 else ''}{int((speed - 1) * 100)}%\"\n",
    "\n",
    "        # Validated the rate\n",
    "        if not (-100 <= int((speed - 1) * 100) <= 100):\n",
    "            raise ValueError(\"Invalid rate. Speed percentage must be between -100% and 100%.\")\n",
    "\n",
    "        # Initialized the TTS engine with the correct parameters\n",
    "        communicate = edge_tts.Communicate(text, voice=voice, rate=rate_percentage)\n",
    "        await communicate.save(output_file)\n",
    "        \n",
    "        print(f\"File saved successfully at: {os.path.abspath(output_file)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during TTS conversion: {e}\")\n",
    "\n",
    "\n",
    "# Usage of the Pipeline\n",
    "async def run_pipeline(audio_file_path, vad_level=2, pitch=1.0, speed=1.0, voice=\"en-US-JennyNeural\"):\n",
    "    \"\"\"\n",
    "    Runs the entire voice query pipeline:\n",
    "    1. Applied VAD to the input audio\n",
    "    2. Transcribed the speech to text using Whisper\n",
    "    3. Inputed the transcription to an LLM to generate a response\n",
    "    4. Converted the LLM response back to speech with adjustable parameters\n",
    "\n",
    "    Parameters:\n",
    "    - audio_file_path: Path to the audio file\n",
    "    - vad_level: VAD aggressiveness (0-3)\n",
    "    - pitch: Pitch adjustment for the TTS conversion\n",
    "    - speed: Speed adjustment for the TTS conversion\n",
    "    - voice: Voice profile to use for TTS conversion\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Load and preprocess audio\n",
    "        print(\"Loading audio...\")\n",
    "        audio, sr = librosa.load(audio_file_path, sr=16000, mono=True)\n",
    "        \n",
    "        # Step 2: Apply Voice Activity Detection (VAD)\n",
    "        print(\"Applying VAD...\")\n",
    "        vad_audio = apply_vad(audio, sr, vad_level)\n",
    "        \n",
    "        # Step 3: Transcribe using Whisper\n",
    "        print(\"Transcribing audio...\")\n",
    "        transcription = transcribe_with_whisper(vad_audio, sr)\n",
    "        print(\"Transcribed Text:\", transcription)\n",
    "        \n",
    "        # Save transcription to a file for verification\n",
    "        with open(\"transcription.txt\", \"w\") as f:\n",
    "            f.write(transcription)\n",
    "        \n",
    "        # Step 4: Query LLM and restrict response to 2 sentences\n",
    "        print(\"Querying LLM...\")\n",
    "        llm_response = query_llm(transcription)\n",
    "        print(\"LLM Response:\", llm_response)\n",
    "        \n",
    "        # Saved the LLM response to a file for verification\n",
    "        with open(\"llm_response.txt\", \"w\") as f:\n",
    "            f.write(llm_response)\n",
    "        \n",
    "        # Step 5: Converted the LLM response to speech\n",
    "        print(\"Converting text to speech...\")\n",
    "        output_file = \"output.mp3\"\n",
    "        # playsound(output_file)\n",
    "        await text_to_speech(llm_response, output_file, pitch=pitch, speed=speed, voice=voice)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the pipeline: {e}\")\n",
    "\n",
    "\n",
    "# Example call to the pipeline\n",
    "audio_file_path = r\"C:\\Users\\sabha\\Downloads\\Music\\audio_input_1.mp3\"\n",
    "await run_pipeline(audio_file_path, vad_level=2, pitch=1.2, speed=1.1, voice=\"en-US-JennyNeural\")\n",
    "# If you want to play the sound:\n",
    "# from playsound import playsound\n",
    "# playsound(r\"C:\\Users\\sabha\\output.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e403b-851d-47bd-b740-b7a0627d8cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
